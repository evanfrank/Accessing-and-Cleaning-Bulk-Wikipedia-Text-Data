# Accessing Bulk Wikipedia Text Data

## Abstract
Wikipedia is one of the largest repositories of human knowledge and offers a huge store of information to mine and use for everything from research to model development. While there are many ways to access the data, one of the most interesting is through accessing the dump files provided by wikipedia which can give access to huge amounts of data quickly. In this example I am processing the full English language wikipedia dump.

## Libaries
Below are the three libarries we will be using for this tutotrial.

* mwxml is for steaming the xml dump
* wikitextparser for cleaning article text
* pprint so we can actually see what we are doing.
